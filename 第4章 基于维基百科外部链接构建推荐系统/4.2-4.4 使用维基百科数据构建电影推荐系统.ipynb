{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T13:51:14.199508Z",
     "start_time": "2021-05-22T13:51:14.196640Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Input, Reshape\n",
    "from keras.layers.merge import Dot\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2　训练电影嵌入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先计数传出链接，将其作为一个快速的方法来看看我们认为的是否合理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T13:51:33.109451Z",
     "start_time": "2021-05-22T13:51:32.761102Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/wp_movies_10k.ndjson') as fin:\n",
    "    movies = [json.loads(l) for l in fin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T13:51:48.456319Z",
     "start_time": "2021-05-22T13:51:48.181641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rotten Tomatoes', 9393),\n",
       " ('Category:English-language films', 5882),\n",
       " ('Category:American films', 5867),\n",
       " ('Variety (magazine)', 5450),\n",
       " ('Metacritic', 5112),\n",
       " ('Box Office Mojo', 4186),\n",
       " ('The New York Times', 3818),\n",
       " ('The Hollywood Reporter', 3553),\n",
       " ('Roger Ebert', 2707),\n",
       " ('Los Angeles Times', 2454)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_counts = Counter()\n",
    "for movie in movies:\n",
    "    link_counts.update(movie[2])\n",
    "link_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们模型的任务是检测电影的维基百科页面是否能找到特定的链接，因此我们需要向模型输入一些标识好的匹配和不匹配的例子。我们只保留出现至少三次的链接，建立一个列表存放有效的（link，movie）对，用于后面快速查找。我们顺便把同样的东西作为集合，也便于后续查找："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T15:32:19.853547Z",
     "start_time": "2021-05-22T15:32:19.341285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(949544, 66913, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_links = [link for link, c in link_counts.items() if c >= 3]\n",
    "link_to_idx = {link: idx for idx, link in enumerate(top_links)}\n",
    "movie_to_idx = {movie[0]: idx for idx, movie in enumerate(movies)}\n",
    "pairs = []\n",
    "for movie in movies:\n",
    "    pairs.extend((link_to_idx[link], movie_to_idx[movie[0]]) for link in movie[2] if link in link_to_idx)\n",
    "pairs_set = set(pairs)\n",
    "len(pairs), len(top_links), len(movie_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T15:33:18.830813Z",
     "start_time": "2021-05-22T15:33:18.787841Z"
    }
   },
   "source": [
    "从原理上讲，我们把link_id和movie_id作为数字，并将它们输入到各自的嵌入层中。嵌入层将为每个可能的输入分配一个embedding_size大小的向量。然后我们将这两个向量的点积设为模型的输出。该模型将学习权重，使得该点积接近于标签。然后，这些权重将电影和链接映射到一个空间中，使得类似的电影最终处于相似的位置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T15:42:03.615894Z",
     "start_time": "2021-05-22T15:42:03.571338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "link (InputLayer)               [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movie (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "link_embedding (Embedding)      (None, 1, 50)        3345650     link[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "movie_embedding (Embedding)     (None, 1, 50)        500000      movie[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_product (Dot)               (None, 1, 1)         0           link_embedding[0][0]             \n",
      "                                                                 movie_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1)            0           dot_product[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 3,845,650\n",
      "Trainable params: 3,845,650\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def movie_embedding_model(embedding_size=50):\n",
    "    link = Input(name='link', shape=(1,))\n",
    "    movie = Input(name='movie', shape=(1,))\n",
    "    link_embedding = Embedding(name='link_embedding',\n",
    "                               input_dim=len(top_links),\n",
    "                               output_dim=embedding_size)(link)\n",
    "    movie_embedding = Embedding(name='movie_embedding',\n",
    "                                input_dim=len(movie_to_idx),\n",
    "                                output_dim=embedding_size)(movie)\n",
    "    dot = Dot(name='dot_product', normalize=True, axes=2)(\n",
    "        [link_embedding, movie_embedding])\n",
    "    merged = Reshape((1,))(dot)\n",
    "    model = Model(inputs=[link, movie], outputs=[merged])\n",
    "    model.compile(optimizer='nadam', loss='mse')\n",
    "    return model\n",
    "\n",
    "\n",
    "model = movie_embedding_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用生成器向模型输入数据。该生成器产生一些由正例和反例组成的批次数据。  \n",
    "我们从（link，movie）对数组中采集正例，然后再填入反例。反例随机抽取，并确保不在pairs_set之中。然后，我们以神经网络期望的格式返回输入/输出元组数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T15:52:41.321753Z",
     "start_time": "2021-05-22T15:52:41.314820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'link': array([48731., 31254.,  1313., 13365., 20558., 32318.,  3801., 32643.,\n",
       "         22418.]),\n",
       "  'movie': array([1854., 5530., 7236., 6238.,  849., 7685., 5874., 7628., 1529.])},\n",
       " array([-1.,  1.,  1., -1., -1., -1., -1., -1.,  1.]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(5)\n",
    "\n",
    "\n",
    "def batchifier(pairs, positive_samples=50, negative_ratio=10):\n",
    "    batch_size = positive_samples * (1 + negative_ratio)\n",
    "    batch = np.zeros((batch_size, 3))\n",
    "    while True:\n",
    "        for idx, (link_id, movie_id) in enumerate(random.sample(pairs, positive_samples)):\n",
    "            batch[idx, :] = (link_id, movie_id, 1)\n",
    "        idx = positive_samples\n",
    "        while idx < batch_size:\n",
    "            movie_id = random.randrange(len(movie_to_idx))\n",
    "            link_id = random.randrange(len(top_links))\n",
    "            if not (link_id, movie_id) in pairs_set:\n",
    "                batch[idx, :] = (link_id, movie_id, -1)\n",
    "                idx += 1\n",
    "        np.random.shuffle(batch)\n",
    "        yield {'link': batch[:, 0], 'movie': batch[:, 1]}, batch[:, 2]\n",
    "\n",
    "\n",
    "next(batchifier(pairs, positive_samples=3, negative_ratio=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T16:05:44.808746Z",
     "start_time": "2021-05-22T15:52:42.922412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1854/1854 - 51s - loss: 0.2466\n",
      "Epoch 2/15\n",
      "1854/1854 - 49s - loss: 0.2270\n",
      "Epoch 3/15\n",
      "1854/1854 - 50s - loss: 0.2198\n",
      "Epoch 4/15\n",
      "1854/1854 - 51s - loss: 0.2170\n",
      "Epoch 5/15\n",
      "1854/1854 - 51s - loss: 0.2153\n",
      "Epoch 6/15\n",
      "1854/1854 - 52s - loss: 0.2143\n",
      "Epoch 7/15\n",
      "1854/1854 - 51s - loss: 0.2137\n",
      "Epoch 8/15\n",
      "1854/1854 - 53s - loss: 0.2132\n",
      "Epoch 9/15\n",
      "1854/1854 - 51s - loss: 0.2129\n",
      "Epoch 10/15\n",
      "1854/1854 - 52s - loss: 0.2126\n",
      "Epoch 11/15\n",
      "1854/1854 - 56s - loss: 0.2123\n",
      "Epoch 12/15\n",
      "1854/1854 - 54s - loss: 0.2124\n",
      "Epoch 13/15\n",
      "1854/1854 - 53s - loss: 0.2121\n",
      "Epoch 14/15\n",
      "1854/1854 - 55s - loss: 0.2120\n",
      "Epoch 15/15\n",
      "1854/1854 - 54s - loss: 0.2119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18825d070>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_samples_per_batch = 512\n",
    "\n",
    "model.fit(\n",
    "    batchifier(pairs, positive_samples=positive_samples_per_batch, negative_ratio=10),\n",
    "    epochs=15,\n",
    "    steps_per_epoch=len(pairs) // positive_samples_per_batch,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T16:23:04.623051Z",
     "start_time": "2021-05-22T16:23:04.615461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 Rogue One 1.0000001\n",
      "19 Interstellar (film) 0.97693247\n",
      "245 Gravity (film) 0.97262806\n",
      "659 Rise of the Planet of the Apes 0.96969795\n",
      "25 Star Wars sequel trilogy 0.9688958\n",
      "86 Tomorrowland (film) 0.9622955\n",
      "62 Fantastic Beasts and Where to Find Them (film) 0.9603083\n",
      "3349 Star Wars: The Force Awakens 0.9598935\n",
      "221 The Dark Knight Trilogy 0.9597953\n",
      "784 Spider-Man 2 0.9572549\n"
     ]
    }
   ],
   "source": [
    "movie = model.get_layer('movie_embedding')\n",
    "movie_weights = movie.get_weights()[0]\n",
    "movie_lengths = np.linalg.norm(movie_weights, axis=1)\n",
    "normalized_movies = (movie_weights.T / movie_lengths).T\n",
    "\n",
    "def similar_movies(movie):\n",
    "    dists = np.dot(normalized_movies, normalized_movies[movie_to_idx[movie]])\n",
    "    closest = np.argsort(dists)[-10:]\n",
    "    for c in reversed(closest):\n",
    "        print(c, movies[c][0], dists[c])\n",
    "\n",
    "similar_movies('Rogue One')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T16:26:31.533279Z",
     "start_time": "2021-05-22T16:26:31.513682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127 George Lucas 1.0\n",
      "3176 Star Wars (film) 0.95481646\n",
      "2707 Star Wars 0.95048773\n",
      "4830 widescreen 0.9459705\n",
      "976 Hugo Award for Best Dramatic Presentation 0.9346279\n",
      "4051 novelization 0.9107668\n",
      "2778 Lucasfilm 0.908279\n",
      "2931 LaserDisc 0.9057042\n",
      "1732 Academy Award for Best Visual Effects 0.90343004\n",
      "2810 film treatment 0.8994031\n"
     ]
    }
   ],
   "source": [
    "link = model.get_layer('link_embedding')\n",
    "link_weights = link.get_weights()[0]\n",
    "link_lengths = np.linalg.norm(link_weights, axis=1)\n",
    "normalized_links = (link_weights.T / link_lengths).T\n",
    "\n",
    "def similar_links(link):\n",
    "    dists = np.dot(normalized_links, normalized_links[link_to_idx[link]])\n",
    "    closest = np.argsort(dists)[-10:]\n",
    "    for c in reversed(closest):\n",
    "        print(c, top_links[c], dists[c])\n",
    "\n",
    "similar_links('George Lucas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3　构建电影推荐系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T16:27:41.849122Z",
     "start_time": "2021-05-22T16:27:41.844299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 50)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = ['Star Wars: The Force Awakens', 'The Martian (film)', 'Tangerine (film)', 'Straight Outta Compton (film)',\n",
    "        'Brooklyn (film)', 'Carol (film)', 'Spotlight (film)']\n",
    "worst = ['American Ultra', 'The Cobbler (2014 film)', 'Entourage (film)', 'Fantastic Four (2015 film)',\n",
    "         'Get Hard', 'Hot Pursuit (2015 film)', 'Mortdecai (film)', 'Serena (2014 film)', 'Vacation (2015 film)']\n",
    "y = np.asarray([1 for _ in best] + [0 for _ in worst])\n",
    "X = np.asarray([normalized_movies[movie_to_idx[movie]]\n",
    "                for movie in best + worst])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T16:27:51.214753Z",
     "start_time": "2021-05-22T16:27:51.205242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在数据集中的所有电影上运行新的分类器，并打印最好和最差的五个："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T16:29:26.654777Z",
     "start_time": "2021-05-22T16:29:26.635385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best:\n",
      "481 The Devil Wears Prada (film) 1.3518291914840432\n",
      "66 Skyfall 1.334615196377807\n",
      "458 Hugo (film) 1.1848986072227856\n",
      "307 Les Misérables (2012 film) 1.1502452556763718\n",
      "3 Spectre (2015 film) 1.1388150583799725\n",
      "worst:\n",
      "1878 The Little Rascals (film) -1.6223719054695447\n",
      "5097 Ready to Rumble -1.5909924369067088\n",
      "1782 Scooby-Doo! WrestleMania Mystery -1.5784702479850679\n",
      "7889 The Comebacks -1.5666192198507929\n",
      "3073 Joe Dirt -1.5574117594165326\n"
     ]
    }
   ],
   "source": [
    "estimated_movie_ratings = clf.decision_function(normalized_movies)\n",
    "best = np.argsort(estimated_movie_ratings)\n",
    "print('best:')\n",
    "for c in reversed(best[-5:]):\n",
    "    print(c, movies[c][0], estimated_movie_ratings[c])\n",
    "\n",
    "print('worst:')\n",
    "for c in best[:5]:\n",
    "    print(c, movies[c][0], estimated_movie_ratings[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4　预测简单的电影属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T16:34:56.559410Z",
     "start_time": "2021-05-22T16:34:56.556144Z"
    }
   },
   "source": [
    "你希望预测简单的电影属性，比如烂番茄评级。  \n",
    "在嵌入模型学习到的向量上使用线性回归模型预测电影的属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们尝试进行烂番茄评级。幸运的是，它们已经存在于我们的数据中，以字符串N%格式存储在movie[-2]中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T16:35:42.876201Z",
     "start_time": "2021-05-22T16:35:42.863072Z"
    }
   },
   "outputs": [],
   "source": [
    "rotten_y = np.asarray([float(movie[-2][:-1]) / 100 for movie in movies if movie[-2]])\n",
    "rotten_X = np.asarray([normalized_movies[movie_to_idx[movie[0]]] for movie in movies if movie[-2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这使我们得到了大约一半电影的数据。让我们训练前面80%的数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T16:36:14.760714Z",
     "start_time": "2021-05-22T16:36:14.752317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_CUT_OFF = int(len(rotten_X) * 0.8)\n",
    "regr = LinearRegression()\n",
    "regr.fit(rotten_X[:TRAINING_CUT_OFF], rotten_y[:TRAINING_CUT_OFF])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们看看模型在剩下20%的数据上的表现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T16:38:12.460440Z",
     "start_time": "2021-05-22T16:38:12.456633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mean square error 0.06'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = (regr.predict(rotten_X[TRAINING_CUT_OFF:]) - rotten_y[TRAINING_CUT_OFF:])\n",
    "'mean square error %2.2f' % np.mean(error ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T16:41:28.508186Z",
     "start_time": "2021-05-22T16:41:28.504676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'数据自身方差 0.09'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = (np.mean(rotten_y[:TRAINING_CUT_OFF]) - rotten_y[TRAINING_CUT_OFF:])\n",
    "'数据自身方差 %2.2f' % np.mean(error ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T16:44:07.443702Z",
     "start_time": "2021-05-22T16:44:07.428047Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 The Martian (film) 10900.0\n",
      "7 List of Marvel Cinematic Universe films 4300.0\n",
      "49 Back to the Future 3900.0\n",
      "71 The Conjuring 2932.0\n",
      "162 Thor (film) 2464.0\n",
      "36 Furious 7 2340.0\n",
      "30 Finding Dory 2187.0\n",
      "1906 Jane Eyre (2011 film) 2068.0\n",
      "19 Interstellar (film) 1670.0\n",
      "2251 An American Werewolf in London 1655.0\n"
     ]
    }
   ],
   "source": [
    "def gross(movie):\n",
    "    v = movie[1].get('gross')\n",
    "    if not v or not ' ' in v:\n",
    "        return None\n",
    "    v, unit = v.split(' ', 1)\n",
    "    unit = unit.lower()\n",
    "    if not unit in ('million', 'billion'):\n",
    "        return None\n",
    "    if not v.startswith('$'):\n",
    "        return None\n",
    "    try:\n",
    "        v = float(v[1:])\n",
    "    except ValueError:\n",
    "        return None\n",
    "    if unit == 'billion':\n",
    "        v *= 1000\n",
    "    return v\n",
    "\n",
    "movie_gross = [gross(m) for m in movies]\n",
    "movie_gross = np.asarray([gr for gr in movie_gross if gr is not None])\n",
    "highest = np.argsort(movie_gross)[-10:]\n",
    "for c in reversed(highest):\n",
    "    print(c, movies[c][0], movie_gross[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T16:44:33.832950Z",
     "start_time": "2021-05-22T16:44:33.826155Z"
    }
   },
   "outputs": [],
   "source": [
    "gross_y = np.asarray([gr for gr in movie_gross if gr])\n",
    "gross_X = np.asarray([normalized_movies[movie_to_idx[movie[0]]]\n",
    "                      for movie, gr in zip(movies, movie_gross) if gr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T16:44:41.150610Z",
     "start_time": "2021-05-22T16:44:41.142969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_CUT_OFF = int(len(gross_X) * 0.8)\n",
    "regr = LinearRegression()\n",
    "regr.fit(gross_X[:TRAINING_CUT_OFF], gross_y[:TRAINING_CUT_OFF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T16:44:48.457293Z",
     "start_time": "2021-05-22T16:44:48.452969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mean square error 8349.41'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = (regr.predict(gross_X[TRAINING_CUT_OFF:]) - gross_y[TRAINING_CUT_OFF:])\n",
    "'mean square error %2.2f' % np.mean(error ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T16:45:06.798044Z",
     "start_time": "2021-05-22T16:45:06.794033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'数据自身方差 14115.59'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = (np.mean(gross_y[:TRAINING_CUT_OFF]) - gross_y[TRAINING_CUT_OFF:])\n",
    "'数据自身方差 %2.2f' % np.mean(error ** 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
